{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be4e5e2",
   "metadata": {},
   "source": [
    "# Module A — Dataset Construction & Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a297a20",
   "metadata": {},
   "source": [
    "1. 2500 data per language\n",
    "2. 5 English and 5 Bangla newspaper\n",
    "3. Crawling done using BeautifulSoup and RSS\n",
    "4. Metadata :<br>\n",
    "    a. title <br>\n",
    "    b. body<br>\n",
    "    c. date<br>\n",
    "    d. url<br>\n",
    "    e. language<br>\n",
    "    f. token number<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927446d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 22 21:50:45 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.97                 Driver Version: 555.97         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8              1W /  140W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67628d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96c5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bda5d",
   "metadata": {},
   "source": [
    "## HTML to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e578986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = html.unescape(s)\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdb48a",
   "metadata": {},
   "source": [
    "## 1. Data Crawling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e121c07",
   "metadata": {},
   "source": [
    "### 1.1 Collect RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43bb223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_from_rss_feeds(rss_feeds, max_docs=200, doc_prefix=\"en\"):\n",
    "    docs = []\n",
    "    seen_urls = set()\n",
    "    doc_i = 0\n",
    "\n",
    "    for rss_url in tqdm(rss_feeds, desc=\"Processing RSS feeds\"):\n",
    "        feed = feedparser.parse(rss_url)\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            url = getattr(entry, \"link\", \"\").strip()\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "\n",
    "            title = getattr(entry, \"title\", \"\").strip()\n",
    "            summary_raw = getattr(entry, \"summary\", \"\").strip()\n",
    "            summary_text = html_to_text(summary_raw)\n",
    "\n",
    "            date = getattr(entry, \"published\", \"\")\n",
    "\n",
    "            seen_urls.add(url)\n",
    "\n",
    "            docs.append({\n",
    "                \"doc_id\": f\"{doc_prefix}_{doc_i:06d}\",\n",
    "                \"title\": html_to_text(title),\n",
    "                \"body\": summary_text,\n",
    "                \"url\": url,\n",
    "                \"date\": date,\n",
    "                \"language\": doc_prefix,\n",
    "                \"token_count\": len(summary_text.split())\n",
    "            })\n",
    "\n",
    "            doc_i += 1\n",
    "\n",
    "            if len(docs) >= max_docs:\n",
    "                return docs\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe40b7",
   "metadata": {},
   "source": [
    "### 1.2 English Newspapers URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8605bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds=[\n",
    "    \"https://www.thedailystar.net/rss.xml\",\n",
    "    \"https://www.dhakatribune.com/feed/\",\n",
    "    \"https://dailynewnation.com/feed/\",\n",
    "    \"https://thebangladeshtoday.com/?feed=rss2\",\n",
    "    \"https://dailyasianage.com/rss/feed.xml\",\n",
    "    \"https://www.thedailystar.net/historical/front-page/rss.xml\",\n",
    "    \"https://www.thedailystar.net/business/rss.xml\",\n",
    "    \"https://www.thedailystar.net/science-tech/rss.xml\",\n",
    "    \"https://www.thedailystar.net/sports/rss.xml\",\n",
    "    \"https://www.thedailystar.net/opinion/rss.xml\",\n",
    "    \"https://www.thedailystar.net/world/rss.xml\",\n",
    "    \"https://www.thedailystar.net/country/rss.xml\",\n",
    "    \"https://www.thedailystar.net/environment/rss.xml\",\n",
    "    \"https://www.thedailystar.net/arts-culture/rss.xml\",\n",
    "    \"https://www.thedailystar.net/magazine/rss.xml\",\n",
    "    \"https://www.thedailystar.net/backpage/rss.xml\",\n",
    "    \"https://www.thedailystar.net/star-weekend/rss.xml\",\n",
    "    \"https://www.thedailystar.net/star-multimedia/rss.xml\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a9120",
   "metadata": {},
   "source": [
    "### 1.3 Data Collecting from English Newspapers in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5f3f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RSS feeds:  89%|████████▉ | 16/18 [00:10<00:01,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 170 new documents. Total: 7432\n"
     ]
    }
   ],
   "source": [
    "saving_path = r\"E:\\DM\\Cross-Lingual-Information-Retrieval-System\\data\"\n",
    "file_path = os.path.join(saving_path, \"document_en.json\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_docs = json.load(f)\n",
    "else:\n",
    "    existing_docs = []\n",
    "\n",
    "existing_urls = {doc[\"url\"] for doc in existing_docs}\n",
    "\n",
    "new_docs = collect_from_rss_feeds(rss_feeds)\n",
    "new_docs = [doc for doc in new_docs if doc[\"url\"] not in existing_urls]\n",
    "\n",
    "start_id = len(existing_docs)\n",
    "for i, doc in enumerate(new_docs):\n",
    "    doc[\"doc_id\"] = f\"en_{start_id + i:06d}\"\n",
    "\n",
    "all_docs = existing_docs + new_docs\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Added {len(new_docs)} new documents. Total: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206834be",
   "metadata": {},
   "source": [
    "### 1.4 Bangla Newspapers URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5daafebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds=[\n",
    "    \"https://www.risingbd.com/rss/rss.xml\",\n",
    "    \"https://bd-journal.com/feed/latest-rss.xml\",\n",
    "    \"https://bangladeshdiplomat.com/feed\",\n",
    "    \"https://www.jagonews24.com/rss/rss.xml\",\n",
    "    \"https://bdpratidin.net/rss/latest-posts\",\n",
    "    \"https://www.kalerkantho.com/rss.xml\",\n",
    "    \"https://www.banglatribune.com/feed/\",\n",
    "    \"https://bangla.thedailystar.net/rss.xml\",\n",
    "    \"https://rss.app/feeds/MeTNrZ6WtYhicYRP.xml\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c32dc",
   "metadata": {},
   "source": [
    "### 1.5 Data Collecting from Bangla Newspapers in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acd9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RSS feeds:  44%|████▍     | 4/9 [00:12<00:15,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 172 new documents. Total: 10910\n"
     ]
    }
   ],
   "source": [
    "saving_path = r\"E:\\DM\\Cross-Lingual-Information-Retrieval-System\\data\"\n",
    "file_path = os.path.join(saving_path, \"document_bn.json\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_docs = json.load(f)\n",
    "else:\n",
    "    existing_docs = []\n",
    "\n",
    "existing_urls = {doc[\"url\"] for doc in existing_docs}\n",
    "\n",
    "new_docs = collect_from_rss_feeds(rss_feeds)\n",
    "new_docs = [doc for doc in new_docs if doc[\"url\"] not in existing_urls]\n",
    "\n",
    "start_id = len(existing_docs)\n",
    "for i, doc in enumerate(new_docs):\n",
    "    doc[\"doc_id\"] = f\"bn_{start_id + i:06d}\"\n",
    "\n",
    "all_docs = existing_docs + new_docs\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Added {len(new_docs)} new documents. Total: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c841dcd",
   "metadata": {},
   "source": [
    "### 1.6 Sitemap-based Web Crawling for Bangla Newspaper (Section-filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a912b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46158200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemaps_from_robots(base_url, timeout=20):\n",
    "    robots_url = base_url.rstrip(\"/\") + \"/robots.txt\"\n",
    "    r = requests.get(robots_url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    sitemaps = []\n",
    "    for line in r.text.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.lower().startswith(\"sitemap:\"):\n",
    "            sitemaps.append(line.split(\":\", 1)[1].strip())\n",
    "    return sitemaps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2b7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sitemap(xml_text):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    tag = root.tag.lower()\n",
    "\n",
    "    ns = \"\"\n",
    "    if \"}\" in root.tag:\n",
    "        ns = root.tag.split(\"}\")[0] + \"}\"\n",
    "\n",
    "    locs = []\n",
    "    if tag.endswith(\"sitemapindex\"):\n",
    "        for sm in root.findall(f\"{ns}sitemap\"):\n",
    "            loc = sm.find(f\"{ns}loc\")\n",
    "            if loc is not None and loc.text:\n",
    "                locs.append(loc.text.strip())\n",
    "    elif tag.endswith(\"urlset\"):\n",
    "        for u in root.findall(f\"{ns}url\"):\n",
    "            loc = u.find(f\"{ns}loc\")\n",
    "            if loc is not None and loc.text:\n",
    "                locs.append(loc.text.strip())\n",
    "\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cfc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_urls_from_sitemaps(base_url, max_urls=2000, timeout=25):\n",
    "    sitemap_urls = get_sitemaps_from_robots(base_url, timeout=timeout)\n",
    "    if not sitemap_urls:\n",
    "        sitemap_urls = [base_url.rstrip(\"/\") + \"/sitemap.xml\"]\n",
    "\n",
    "    seen_sitemaps = set()\n",
    "    seen_urls = set()\n",
    "\n",
    "    queue = list(sitemap_urls)\n",
    "\n",
    "    with tqdm(total=max_urls, desc=f\"Sitemap URLs {base_url}\") as pbar:\n",
    "        while queue and len(seen_urls) < max_urls:\n",
    "            sm_url = queue.pop(0)\n",
    "            if sm_url in seen_sitemaps:\n",
    "                continue\n",
    "            seen_sitemaps.add(sm_url)\n",
    "\n",
    "            try:\n",
    "                r = requests.get(sm_url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r.raise_for_status()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            locs = parse_sitemap(r.text)\n",
    "\n",
    "            if locs and locs[0].endswith(\".xml\"):\n",
    "                for nxt in locs:\n",
    "                    if nxt not in seen_sitemaps:\n",
    "                        queue.append(nxt)\n",
    "            else:\n",
    "                for u in locs:\n",
    "                    if u.startswith(base_url) and u not in seen_urls:\n",
    "                        seen_urls.add(u)\n",
    "                        pbar.update(1)\n",
    "                        if len(seen_urls) >= max_urls:\n",
    "                            break\n",
    "\n",
    "    return list(seen_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c7cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "\n",
    "def extract_article_text(url, timeout=25):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if not downloaded:\n",
    "            return None\n",
    "        text = trafilatura.extract(\n",
    "            downloaded,\n",
    "            include_comments=False,\n",
    "            include_tables=False\n",
    "        )\n",
    "        if not text:\n",
    "            return None\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51e12e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "def build_dataset_from_sitemap_section_append(\n",
    "    section_url,\n",
    "    language,\n",
    "    existing_json_path,\n",
    "    max_new_docs=200,\n",
    "    sleep_sec=0.8\n",
    "):\n",
    "    p = urlparse(section_url)\n",
    "    base_url = f\"{p.scheme}://{p.netloc}\"\n",
    "    section_path = p.path.rstrip(\"/\")\n",
    "    if section_path == \"\":\n",
    "        section_path = \"/\"\n",
    "\n",
    "    if os.path.exists(existing_json_path):\n",
    "        with open(existing_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_docs = json.load(f)\n",
    "    else:\n",
    "        existing_docs = []\n",
    "\n",
    "    existing_urls = set(d.get(\"url\", \"\").strip() for d in existing_docs if d.get(\"url\"))\n",
    "    existing_count = len(existing_docs)\n",
    "\n",
    "    all_urls = collect_urls_from_sitemaps(base_url, max_urls=max_new_docs * 50)\n",
    "    section_urls = [u for u in all_urls if f\"{base_url}{section_path}/\" in u]\n",
    "\n",
    "    new_docs = []\n",
    "    doc_i = existing_count\n",
    "\n",
    "    for url in tqdm(section_urls, desc=f\"Append from {section_url}\"):\n",
    "        if url in existing_urls:\n",
    "            continue\n",
    "\n",
    "        text = extract_article_text(url)\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "        if len(text.split()) < 50:\n",
    "            continue\n",
    "\n",
    "        title = \"\"\n",
    "        try:\n",
    "            html = requests.get(url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"}).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            h1 = soup.find(\"h1\")\n",
    "            if h1:\n",
    "                title = h1.get_text(\" \", strip=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not title:\n",
    "            title = text.split(\".\")[0][:120]\n",
    "\n",
    "        d = {\n",
    "            \"doc_id\": f\"{language}_{doc_i:06d}\",\n",
    "            \"title\": title,\n",
    "            \"body\": text,\n",
    "            \"url\": url,\n",
    "            \"date\": \"\",\n",
    "            \"language\": language,\n",
    "            \"token_count\": len(text.split())\n",
    "        }\n",
    "\n",
    "        existing_docs.append(d)\n",
    "        existing_urls.add(url)\n",
    "        new_docs.append(d)\n",
    "        doc_i += 1\n",
    "\n",
    "        if len(new_docs) >= max_new_docs:\n",
    "            break\n",
    "\n",
    "    with open(existing_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Existing docs (before):\", existing_count)\n",
    "    print(\"New docs added:\", len(new_docs))\n",
    "    print(\"Total docs (after):\", len(existing_docs))\n",
    "    if new_docs:\n",
    "        print(\"Example new:\", new_docs[0][\"title\"], \"|\", new_docs[0][\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25844027",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_urls = [\n",
    "#prothom alo\n",
    "\"https://www.prothomalo.com/bangladesh\",\n",
    "\"https://www.prothomalo.com/politics\",\n",
    "\"https://www.prothomalo.com/world\",\n",
    "\"https://www.prothomalo.com/business\",\n",
    "\"https://www.prothomalo.com/sports\",\n",
    "\"https://www.prothomalo.com/entertainment\",\n",
    "\n",
    "#dhaka post\n",
    "\"https://www.dhakapost.com/national\",\n",
    "\"https://www.dhakapost.com/politics\",\n",
    "\"https://www.dhakapost.com/economy\",\n",
    "\"https://www.dhakapost.com/international\",\n",
    "\"https://www.dhakapost.com/sports\",\n",
    "\"https://www.dhakapost.com/entertainment\",\n",
    "\n",
    "\n",
    "#jugantor\n",
    "\"https://www.jugantor.com/national\",\n",
    "\"https://www.jugantor.com/politics\",\n",
    "\"https://www.jugantor.com/economics\",\n",
    "\"https://www.jugantor.com/international\",\n",
    "\"https://www.jugantor.com/entertainment\",\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0d8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Link:  https://www.prothomalo.com/bangladesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sitemap URLs https://www.prothomalo.com:  88%|████████▊ | 22111/25000 [01:20<00:10, 274.49it/s]\n",
      "Append from https://www.prothomalo.com/bangladesh:  14%|█▍        | 1087/7822 [39:22<3:14:02,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "for url in selected_urls[0:17]:\n",
    "    print(\"Processing Link: \",url)\n",
    "    build_dataset_from_sitemap_section_append(\n",
    "    section_url=url,\n",
    "    language=\"bn\",\n",
    "    existing_json_path=r\"E:\\DM\\Cross-Lingual-Information-Retrieval-System\\data\\document_bn.json\",\n",
    "    max_new_docs=500,\n",
    "    sleep_sec=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd4dbe6",
   "metadata": {},
   "source": [
    "### 1.7 Sitemap-based Web Crawling for English Newspaper (Section-filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56075b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_url(u):\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    u = u.strip().split(\"#\", 1)[0]\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c533416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _same_domain(u, base_url):\n",
    "    try:\n",
    "        return urlparse(u).netloc == urlparse(base_url).netloc\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00512a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_html(url, timeout=25):\n",
    "    r = requests.get(url, timeout=timeout, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    })\n",
    "    r.raise_for_status()\n",
    "    return r.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_article_text_requests(url, timeout=25):\n",
    "    try:\n",
    "        html = _get_html(url, timeout=timeout)\n",
    "        text = trafilatura.extract(html, include_comments=False, include_tables=False)\n",
    "        if not text:\n",
    "            return None\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51487591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_title_h1(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1:\n",
    "            return h1.get_text(\" \", strip=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc90ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_links_from_listing(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = _norm_url(a.get(\"href\", \"\"))\n",
    "        if not href:\n",
    "            continue\n",
    "        abs_url = urljoin(base_url, href)\n",
    "        links.append(abs_url)\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097201f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _looks_like_article(u, base_url):\n",
    "    if not _same_domain(u, base_url):\n",
    "        return False\n",
    "    path = urlparse(u).path.lower().strip(\"/\")\n",
    "    if not path:\n",
    "        return False\n",
    "    bad = [\"category\", \"tag\", \"author\", \"page\", \"search\", \"privacy\", \"terms\", \"contact\", \"about\", \"login\", \"signup\"]\n",
    "    if any(f\"/{b}/\" in \"/\" + path + \"/\" for b in bad):\n",
    "        return False\n",
    "    if path.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".pdf\", \".mp4\")):\n",
    "        return False\n",
    "    if len(path.split(\"/\")) == 1 and len(path) < 8:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56818188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_next_page(html, current_url, base_url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rel_next = soup.find(\"link\", rel=lambda x: x and \"next\" in x.lower())\n",
    "    if rel_next and rel_next.get(\"href\"):\n",
    "        return urljoin(base_url, _norm_url(rel_next[\"href\"]))\n",
    "\n",
    "    a_next = soup.find(\"a\", rel=lambda x: x and \"next\" in x.lower())\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        return urljoin(base_url, _norm_url(a_next[\"href\"]))\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        txt = a.get_text(\" \", strip=True).lower()\n",
    "        if txt in [\"next\", \"next >\", \"older\", \"older posts\", \"›\", \"»\"]:\n",
    "            return urljoin(base_url, _norm_url(a[\"href\"]))\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_from_section_pages_append(\n",
    "    section_urls,\n",
    "    language,\n",
    "    existing_json_path,\n",
    "    max_new_docs=200,\n",
    "    max_pages_per_section=5,\n",
    "    sleep_sec=1.0,\n",
    "    min_tokens=50\n",
    "):\n",
    "    if os.path.exists(existing_json_path):\n",
    "        with open(existing_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_docs = json.load(f)\n",
    "    else:\n",
    "        existing_docs = []\n",
    "\n",
    "    existing_urls = set(d.get(\"url\", \"\").strip() for d in existing_docs if d.get(\"url\"))\n",
    "    doc_i = len(existing_docs)\n",
    "\n",
    "    base_url = f\"{urlparse(section_urls[0]).scheme}://{urlparse(section_urls[0]).netloc}\"\n",
    "\n",
    "    new_docs = []\n",
    "\n",
    "    for section_url in section_urls:\n",
    "        current = section_url\n",
    "        for _ in range(max_pages_per_section):\n",
    "            try:\n",
    "                listing_html = _get_html(current)\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "            links = _extract_links_from_listing(listing_html, base_url)\n",
    "            article_links = []\n",
    "            for u in links:\n",
    "                if _looks_like_article(u, base_url):\n",
    "                    article_links.append(u)\n",
    "            article_links = list(dict.fromkeys(article_links))\n",
    "\n",
    "            for url in tqdm(article_links, desc=f\"Scrape {current}\"):\n",
    "                if url in existing_urls:\n",
    "                    continue\n",
    "\n",
    "                text = _extract_article_text_requests(url)\n",
    "                time.sleep(sleep_sec)\n",
    "\n",
    "                if not text:\n",
    "                    continue\n",
    "                if len(text.split()) < min_tokens:\n",
    "                    continue\n",
    "\n",
    "                title = \"\"\n",
    "                try:\n",
    "                    html_article = _get_html(url)\n",
    "                    title = _extract_title_h1(html_article)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if not title:\n",
    "                    title = text.split(\".\")[0][:120]\n",
    "\n",
    "                d = {\n",
    "                    \"doc_id\": f\"{language}_{doc_i:06d}\",\n",
    "                    \"title\": title,\n",
    "                    \"body\": text,\n",
    "                    \"url\": url,\n",
    "                    \"date\": \"\",\n",
    "                    \"language\": language,\n",
    "                    \"token_count\": len(text.split())\n",
    "                }\n",
    "\n",
    "                existing_docs.append(d)\n",
    "                existing_urls.add(url)\n",
    "                new_docs.append(d)\n",
    "                doc_i += 1\n",
    "\n",
    "                if len(new_docs) >= max_new_docs:\n",
    "                    with open(existing_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(existing_docs, f, ensure_ascii=False, indent=2)\n",
    "                    print(\"New docs added:\", len(new_docs))\n",
    "                    print(\"Total docs:\", len(existing_docs))\n",
    "                    if new_docs:\n",
    "                        print(\"Example new:\", new_docs[0][\"title\"], \"|\", new_docs[0][\"url\"])\n",
    "                    return\n",
    "\n",
    "            nxt = _find_next_page(listing_html, current, base_url)\n",
    "            if not nxt or nxt == current:\n",
    "                break\n",
    "            current = nxt\n",
    "\n",
    "    with open(existing_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"New docs added:\", len(new_docs))\n",
    "    print(\"Total docs:\", len(existing_docs))\n",
    "    if new_docs:\n",
    "        print(\"Example new:\", new_docs[0][\"title\"], \"|\", new_docs[0][\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b619cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_urls = [\n",
    "\"https://www.newagebd.net/articlelist/41/bangladesh\",\n",
    "\"https://www.newagebd.net/articlelist/29/business-economy\",\n",
    "\"https://www.newagebd.net/articlelist/31/world\",\n",
    "\"https://www.newagebd.net/articlelist/22/sports\",\n",
    "\"https://www.newagebd.net/articlelist/25/editorial\",\n",
    "\"https://www.newagebd.net/articlelist/27/environment\",\n",
    "\"https://dailynewnation.com/category/todays-news/national\",\n",
    "\"https://dailynewnation.com/category/todays-news/business-economy\",\n",
    "\"https://dailynewnation.com/category/todays-news/international\",\n",
    "\"https://dailynewnation.com/category/todays-news/sports\",\n",
    "\"https://dailynewnation.com/category/todays-news/entertainment\",\n",
    "\"https://dailynewnation.com/category/news-buzz\",\n",
    "\"https://www.daily-sun.com/bangladesh\",\n",
    "\"https://www.daily-sun.com/business\",\n",
    "\"https://www.daily-sun.com/world\",\n",
    "\"https://www.daily-sun.com/sports\",\n",
    "\"https://www.daily-sun.com/entertainment\",\n",
    "\"https://www.daily-sun.com/technology\",\n",
    "\"https://www.dhakatribune.com/latest-news\",\n",
    "\"https://www.dhakatribune.com/politics\",\n",
    "\"https://www.dhakatribune.com/business\",\n",
    "\"https://www.dhakatribune.com/world\",\n",
    "\"https://www.dhakatribune.com/sport\",\n",
    "\"https://www.dhakatribune.com/showtime\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Link:  https://www.newagebd.net/articlelist/41/bangladesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrape https://www.newagebd.net/articlelist/41/bangladesh: 100%|██████████| 86/86 [00:56<00:00,  1.52it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/bangladesh?page=2: 100%|██████████| 91/91 [00:58<00:00,  1.57it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/bangladesh?page=3: 100%|██████████| 91/91 [00:58<00:00,  1.57it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/bangladesh?page=4: 100%|██████████| 91/91 [00:57<00:00,  1.57it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/bangladesh?page=5: 100%|██████████| 91/91 [00:59<00:00,  1.52it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/politics?page=6: 100%|██████████| 91/91 [00:59<00:00,  1.52it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/politics?page=7: 100%|██████████| 91/91 [01:00<00:00,  1.50it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/politics?page=8: 100%|██████████| 91/91 [00:59<00:00,  1.53it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/politics?page=9: 100%|██████████| 91/91 [00:56<00:00,  1.62it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/41/politics?page=10: 100%|██████████| 91/91 [01:00<00:00,  1.50it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/29/business-economy: 100%|██████████| 93/93 [01:01<00:00,  1.50it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/29/business-economy?page=2: 100%|██████████| 94/94 [01:01<00:00,  1.53it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/29/business-economy?page=3: 100%|██████████| 94/94 [01:01<00:00,  1.53it/s]\n",
      "Scrape https://www.newagebd.net/articlelist/29/business-economy?page=4:  74%|███████▍  | 70/94 [00:13<00:04,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New docs added: 200\n",
      "Total docs: 7262\n",
      "Example new: Television | https://www.newagebd.net/articlelist/80/television\n",
      "New docs added: 175\n",
      "Total docs: 9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for url in selected_urls[0:1]:\n",
    "    print(\"Processing Link: \",url)\n",
    "    build_dataset_from_section_pages_append(\n",
    "    section_urls=selected_urls,\n",
    "    language=\"en\",\n",
    "    existing_json_path=r\"E:\\DM\\Cross-Lingual-Information-Retrieval-System\\data\\document_en.json\",\n",
    "    #max_new_docs=500,\n",
    "    max_pages_per_section=10,\n",
    "    sleep_sec=1.0\n",
    ")\n",
    "    print(\"New docs added:\", len(new_docs))\n",
    "    print(\"Total docs:\", len(existing_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895911b",
   "metadata": {},
   "source": [
    "Purpose\n",
    "\n",
    "1. Gain exposure to real-world, messy data\n",
    "2. Understand indexing fundamentals\n",
    "3. Create a foundation for multilingual search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b7205",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52100e31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
